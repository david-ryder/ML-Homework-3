{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Name Here: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3 LogisticRegression, Multi-layer Perceptron (MLP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data and perform general EDA (3+2+1+3+6 = 15pts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 import libraries: numpy, matplotlib.pyplot and pandas. (1+1+1 = 3pts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import other libraries below as needed in any block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 import the data to a pandas dataframe and show the count of rows and columns (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line if you are running this code in google colab and have uploaded the dataset to your drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Show if any column has null values. (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 We will use the 'quality' column as our target label. Print the count of each label in the dataset (in percentage). Mention which quality scores are among top 3 by their counts. (3pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Observe the correlation matrix for the columns. Name the pairs of columns with highest positive and highest negative correlations. (2+4 = 6pts) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import and use seaborn here to visualize correlation matrix in a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection and Preprocessing (2+2+3+3 = 10pts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Drop the 'color' attribute from the dataframe. (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Assign the 'quality' column to target label y, and all other columns to attribute matrix X (1+1 = 2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Use scikitlearn's Standard Scaler to scale the feature matrix X. (3pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Split Dataset into Training and Test set. (3 pts)\n",
    "Use 80:20 split for training and test. Also use stratified sampling to ensure balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression from scratch (10+10+10+25+20 = 75pts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will write a logistic regression algorithm from scratch using python and numpy. You will first write some helper functions for transforming label encoding to one-hot encoding, for measuring the cross-entropy loss and predicting labels from test data. These functions will later be used in the logistic regression training and testing.   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The target label in our dataset is the 'quality' column. It consists of categorical values, label-encoded with 7 values in the range [3-9]. To perform a multiclass logistic regression, you need a function that takes a label matrix and converts it to One-hot encoded labels. (10pts)\n",
    "\n",
    "Write the function below following the docstring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(y):\n",
    "    \"\"\"\n",
    "    Converts an array of labels to one-hot encoding.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : numpy.ndarray\n",
    "        An array of labels. It must have a shape of (n_samples, ).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        A matrix of one-hot encoding. It has a shape of (n_samples, n_classes).\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 For multi-class classification, you also need to measure cross-entropy loss. Cross-entropy loss is measured by the following formula:\n",
    "$\\text{Cross-Entropy Loss} = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^n y_{ij} \\log \\hat{y}_{ij}$\n",
    "\n",
    "Write a function that takes two matrices: one matrix of true labels $y_{ij}$, and one matrix of the predicted labels by your model $\\hat{y}_{ij}$ (you are about to write the model function soon below), and returns the computed loss. **(10pts)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss between the true labels and predicted labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : numpy array\n",
    "        Array of true labels with shape (m, n_classes).\n",
    "    y_pred : numpy array\n",
    "        Array of predicted labels with shape (m, n_classes).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cross-entropy loss between y_true and y_pred.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function assumes that the labels are one-hot encoded.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Write a function for softmax activation to compute probabilities from the linear score $z_i$. Softmax function uses the following formula:\n",
    "$$\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{k}e^{z_j}} , \\text{for } i=1,2,3,...,k $$\n",
    "where $z_i$ is the linear layer score for i-th input.   \n",
    "\n",
    "**(10pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(scores):\n",
    "    \"\"\"\n",
    "    Compute the softmax of the given scores.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    scores : numpy.ndarray\n",
    "        A 2D numpy array of shape (m, n), where m is the number of samples and n is the number of classes.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    probs : numpy.ndarray\n",
    "        A 2D numpy array of shape (m, n) containing the probabilities of each sample belonging to each class.\n",
    "    \"\"\"\n",
    "       \n",
    "    # TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Model development (20+5 = 25pts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.1** Write a function for the logistic regression algorithm. You will use the functions you have written above in this module. Recall that with gradient descent optimization, the computation loop goes as:\n",
    "For each epoch:\n",
    "1. Compute $scores = X_{train} W + b$ \n",
    "2. Compute probability = softmax(scores)\n",
    "3. Compute Cross-entropy Loss\n",
    "4. Compute gradients of the loss with respect to the weights and bias.\n",
    "\n",
    "    Partial derivative of Loss with respect to Weight matrix:\n",
    "    $$\\frac{\\partial L}{\\partial W} = -\\frac{1}{m} X^T \\left(y - \\hat{y}\\right)$$\n",
    "\n",
    "    Partial derivative of Loss with respect to Bias matrix:\n",
    "    $$\\frac{\\partial L}{\\partial b_j} = -\\frac{1}{m} \\sum_{i=1}^m \\left(y_{ij} - p_{ij}\\right) $$\n",
    "\n",
    "5. Update the weights and biases\n",
    "    $$ W = W - (learning\\ rate * weight\\ gradient)  $$\n",
    "    $$ b = b - (learning\\ rate * weight\\ gradient)  $$\n",
    "\n",
    "**(20pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, y_train, ):\n",
    "    \"\"\"\n",
    "    Performs logistic regression using softmax activation and gradient descent optimizer to classify the input data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : numpy.ndarray\n",
    "        The input training data of shape (num_samples, num_features).\n",
    "    y_train : numpy.ndarray\n",
    "        The training labels of shape (num_samples,).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    W : numpy.ndarray\n",
    "        The learned weights of shape (num_features, num_classes).\n",
    "    b : numpy.ndarray\n",
    "        The learned bias of shape (1, num_classes).\n",
    "    loss_list : list\n",
    "        The list of loss values at each epoch during training.\n",
    "    \"\"\"\n",
    "\n",
    "    # get the number of samples and features from X_train (2pts)\n",
    "    # TODO\n",
    "    \n",
    "    # convert training labels to one-hot encoded labels (2pts)\n",
    "    # TODO\n",
    "\n",
    "    # get the number of target classes from y_train (2pts)\n",
    "    # TODO\n",
    "\n",
    "    # initialize the weights and bias with numpy arrays of zeros (1+1 = 2pts)\n",
    "    # TODO\n",
    "\n",
    "    # set hyperparameters (1+1 = 2pts)\n",
    "    ## set a learning rate\n",
    "    # TODO\n",
    "    ## set the max number of epochs you want to train for\n",
    "    # TODO\n",
    "    \n",
    "    ## initialize a list to store the loss values (1pt)\n",
    "    # TODO\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Write a for loop over epochs.\n",
    "    In each epoch:\n",
    "        compute the score for each class, \n",
    "        compute the softmax probabilities, \n",
    "        compute the cross-entropy loss, \n",
    "        compute the gradients of the loss with respect to the weights and bias, \n",
    "        update the weights and bias using the gradients and the learning rate.\n",
    "    '''\n",
    "    # (9pts)\n",
    "    # TODO\n",
    "        # compute the score (Z) for each class. \n",
    "        # TODO\n",
    "        \n",
    "        # calculate the softmax probabilities\n",
    "        # TODO\n",
    "\n",
    "        # compute the cross-entropy loss\n",
    "        # TODO\n",
    "\n",
    "        # compute the gradients of the loss with respect to the weights and bias\n",
    "        # TODO\n",
    "        \n",
    "        # update the weights and bias using the gradients and the learning rate\n",
    "        # TODO\n",
    "\n",
    "        # For tracking progress, print the loss every 100 epochs\n",
    "        # TODO\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.2** Now that you have the model and the helper function, train the model with your prepared training dataset. Then plot the training loss curve. **(5pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "# TODO\n",
    "\n",
    "# plot the loss curve\n",
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Evaluation (10+1+4+3+2 = 20pts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to predict the labels of X_test using the model trained above and return the predicted labels as categorical values similar to train labels. **(10pts)**\n",
    "\n",
    "Note: By default, the predicted labels may return values within [0-6] instead of the original labels [3-9]. This can happen due to using argmax() to get label encoding from one-hot encoding when predicting test labels. You can add a simple offset to the argmax outputs of predicted labels to shift them to the range of [3-9]. This is important to note as you may see a very low accuracy even with a good model if true labels and predicted labels are not associated with the same categorical scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, W, b):\n",
    "    '''\n",
    "    X_test: a numpy array of testing features\n",
    "    W: a numpy array of weights\n",
    "    b: a numpy array of bias\n",
    "    return: a numpy array of one-hot encoded labels\n",
    "    '''\n",
    "    # compute the scores\n",
    "    # TODO\n",
    "\n",
    "    # compute the probabilities\n",
    "    # TODO\n",
    "\n",
    "    # get the predicted labels\n",
    "    # TODO\n",
    "\n",
    "    # return the predicted labels\n",
    "    # TODO\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.1** import accuracy_score, confusion_matrix, precision_score, recall_score and f1_score from scikitlearn **(1pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.2** Print the prediction scores on test data in terms of accuracy, precision and recall score. **(4pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.3** Generate and visualize the confusion matrix. You can use seaborn heatmap to visualize a heatmap of the confusion matrix. **(3pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.4** Which labels seem to be harder to classify for this model? **(2pts)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/ # TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Scikitlearn Logistic Regression (4+2+7+2 = 15pts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Import LogisticRegressionCV module and create an instance of it. (4pts)\n",
    "Use the following parameters:\n",
    "1. 5-fold crossvalidation, \n",
    "2. lbfgs solver, \n",
    "3. run for 1000 iterations \n",
    "4. use 'multinomial' for choosing softmax classifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train the model and predict the labels for test set. (2pts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Compute the accuracy, precision, recall and F1 scores. Also visualize the confusion matrix. (1+1+1+1+3 =7pts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Did your previous model (built from scratch) and the model from scikitlearn library produced similar evaluation scores? (2pts)\n",
    "\n",
    "Explain (in <100 words) the similarity and dissimilarity in score. What changed, what didn't, and why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/ # TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using Scikitlearn Multi-layer Perceptron (16+19 = 35pts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 MLP with one hidden layer and stochastic gradient descent optimizer (4+2+1+7+2 = 16pts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1.1** Import MLPClassifier from scikitlearn's neural_network library. Then create a model instance of this classifier.\n",
    "\n",
    "Use the following parameters: \n",
    "1. One hidden layer with 100 neurons, \n",
    "2. ReLu activation, \n",
    "3. Stochastic Gradient Descent solver\n",
    "4. Learning rate = 0.01 (or you can continue to use your previous learning rate)\n",
    "5. No of epoch = 1000\n",
    "\n",
    "Also, set random_state to a fixed value so that your result is reproducible. **(4pts)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1.2** Train the model with training set.\n",
    "Then predict the labels for test set. **(2pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1.3** Plot the loss curve **(1pt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1.4** Print the accuracy, precision, recall and F1 scores. Also show the confusion matrix. **(1+1+1+1+3 = 7pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1.5** Did you see any change in scores after using the MLP classifier?\n",
    "\n",
    "In <100 words, describe your observation. **(2pts)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/ # TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 MLP with two hidden layer and adam optimizer (4+2+1+7+2+3 = 19pts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2.1** Import MLPClassifier from scikitlearn's neural_network library. Then create a model instance of this classifier.\n",
    "\n",
    "Use the following parameters: \n",
    "1. **Two** hidden layers with 100 neurons, \n",
    "2. ReLu activation, \n",
    "3. Adam solver (this is an advanced optimizer which we did not cover in class. However the usage is quite straightforward)\n",
    "4. Learning rate = 0.01 (or you can continue to use your previous learning rate)\n",
    "5. No of epoch = 1000\n",
    "\n",
    "Also, set random_state to a fixed value so that your result is reproducible. **(4pts)** \n",
    "\n",
    "(Refer to the scikitlearn documentation for clarification on the parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2.2** Train the model with training set.\n",
    "Then predict the labels for test set. **(2pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2.3** Plot the loss curve **(1pt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2.4** Print the accuracy, precision, recall and F1 scores. Also show the confusion matrix. **(1+1+1+1+3 = 7pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2.5** Describe any difference in the loss curve from MLP with one hidden layer and MLP with two hidden layer. **(2pts)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/ # TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2.6** Did you see any change in scores between the two MLP models?\n",
    "\n",
    "In <100 words, describe your observation. **(3pts)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/ # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
